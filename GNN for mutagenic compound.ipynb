{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2, Grossrieder Wanchai 301436 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________\n",
    "\n",
    "## The different convolution methods\n",
    "It takes the a matrix of size num node, output features and return it back with the new features as shown in this picture :\n",
    "\n",
    "\n",
    "each row of the matrix represent a node and each column a feature.\n",
    "\n",
    "#### we have implemented 3 different kinds of convolutions : Graph convolution, GraphSAGE, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, aggregation = \"mean\", activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the modified graph convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): Number of input node features.\n",
    "            out_features (int): Number of output node features.\n",
    "            activation (nn.Module or callable): Activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Activation function to apply\n",
    "        self.activation = activation\n",
    "        # Infeatures: number of input features per node\n",
    "        self.in_features = in_features\n",
    "        # Outfeatures: number of output features per node\n",
    "        self.out_features = out_features\n",
    "        # Aggregation function: which is applied to aggregate\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "        # Learnable weight matrices, using linear\n",
    "        self.weight = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.bias = nn.Linear(in_features, out_features, bias=False)  # Adding a learnable bias term\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform node-wise graph convolution operation for one node at a time.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after node-wise graph convolution, shape (1, out_features).\n",
    "        \"\"\"\n",
    "        # Get the number of nodes\n",
    "        num_nodes = x.size(0)\n",
    "         #initialize the output features returned by the function\n",
    "        output_features = torch.zeros(num_nodes, self.out_features, device=x.device)\n",
    "\n",
    "        \n",
    "        for node_idx in range(num_nodes):\n",
    "            # Calculate neighbors\n",
    "            neighbors = adj[node_idx].nonzero().squeeze(dim=1)\n",
    "            # Aggregate neighbor features\n",
    "            if neighbors.numel() > 1:\n",
    "                # Aggregate neighbor features if the node has neighbors\n",
    "                if self.aggregation == \"mean\" :\n",
    "                    x_agg = torch.mean(x[neighbors], dim=0, keepdim=True)\n",
    "                elif self.aggregation == \"max\" :\n",
    "                    x_agg,_ = torch.max(x[neighbors], dim=0, keepdim=True)\n",
    "                else :\n",
    "                    raise ValueError(\"Aggregation type not supported.\")\n",
    "            else:\n",
    "                # If the node has no neighbors, set the aggregated features to zero\n",
    "                x_agg = torch.zeros_like(x[node_idx]).unsqueeze(dim=0)\n",
    "            \n",
    "            # Calculate the updated node features\n",
    "            w_xt = self.weight(x_agg)\n",
    "            b_xt = self.bias(x[node_idx])\n",
    "            node_output = w_xt + b_xt\n",
    "            \n",
    "            \n",
    "\n",
    "            # Apply activation function (e.g., ReLU)\n",
    "            if self.activation is not None:\n",
    "                node_output = self.activation(node_output).squeeze()\n",
    "\n",
    "            # Save the updated node features\n",
    "            output_features[node_idx] = node_output\n",
    "\n",
    "        return output_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, aggregation =\"mean\", activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the GraphSAGEConv layer.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Number of input features.\n",
    "            out_features (int): Number of output features.\n",
    "            aggregation (str): Aggregation method to use\n",
    "            activation (callable): Activation function to apply after the linear transformation.\n",
    "        \"\"\"\n",
    "        super(GraphSAGEConv, self).__init__()\n",
    "        # Infeatures: number of input features per node\n",
    "        self.in_features = in_features\n",
    "        # Outfeatures: number of output features per node\n",
    "        self.out_features = out_features\n",
    "        # Activation function : which is applied to the updated node features\n",
    "        self.activation = activation\n",
    "        # Aggregation function: which is applied to aggregate the neighbor information\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "        # Initialize the linear weights\n",
    "        self.weights = nn.Linear(2 * in_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        # Get the number of nodes\n",
    "        num_nodes = x.size(0)\n",
    "        #initialize the output features returned by the function\n",
    "        output_features = torch.zeros(num_nodes, self.out_features, device=x.device)\n",
    "\n",
    "        for node_idx in range(num_nodes):\n",
    "            # Get the neighbors of the current node based on the adjacency matrix\n",
    "            neighbors = adj[node_idx].nonzero().squeeze(dim=1)\n",
    "\n",
    "            # Handle isolated nodes: If no neighbors, use the node's own features\n",
    "            if neighbors.numel() == 0:\n",
    "                x_agg = x[node_idx]\n",
    "            else:\n",
    "                # Aggregate neighbor features\n",
    "                if self.aggregation == \"mean\" :\n",
    "                    x_agg = torch.mean(x[neighbors], dim=0)\n",
    "                elif self.aggregation == \"max\" :\n",
    "                    x_agg,_ = torch.max(x[neighbors], dim=0)\n",
    "                else :\n",
    "                    raise ValueError(\"Aggregation type not supported.\")\n",
    "                \n",
    "            # Concatenate the original node features with the aggregated node features\n",
    "            x_concat = torch.cat([x[node_idx], x_agg], dim=0)\n",
    "\n",
    "            # Apply the weights\n",
    "            x_out = self.weights(x_concat)\n",
    "\n",
    "            # Apply activation function (e.g., ReLU)\n",
    "            if self.activation:\n",
    "                x_out = self.activation(x_out)\n",
    "\n",
    "            # Save the output features\n",
    "            output_features[node_idx] = x_out\n",
    "\n",
    "        return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features, aggregation = \"sum\", activation=None):# according to the statements, the aggregation should be sum\n",
    "        \n",
    "        super(AttentionConv, self).__init__()\n",
    "        # Initialize the layer parameters\n",
    "        # Infeatures: number of input features per node\n",
    "        self.in_features = in_features\n",
    "        # Outfeatures: number of output features per node\n",
    "        self.out_features = out_features\n",
    "        # Activation function : which is applied to the updated node features\n",
    "        self.activation = activation\n",
    "        # Aggregation function: which is applied to aggregate the neighbor information\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "        # Initialize the attention mechanism parameters\n",
    "        self.weights = nn.Linear(in_features, out_features)\n",
    "        # Attention mechanism parameters\n",
    "        self.attention_s_t = nn.Linear(2 * out_features, 1) #correspond to S.T\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation with attention mechanism.\n",
    "        x: Node features, shape [N, in_features]\n",
    "        adj: Adjacency matrix, shape [N, N]\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        out: Updated node features, shape [N, out_features]\n",
    "        \"\"\"\n",
    "        # Linearly transform node features\n",
    "        x_transformed = self.weights(x)  # Shape: (N, out_features)\n",
    "        num_nodes = x.size(0)\n",
    "        #initialize the output features returned by the function\n",
    "        output_features = torch.zeros(num_nodes, self.out_features, device=x.device)\n",
    "\n",
    "        # Check if all nodes are connected to at least one other node\n",
    "        if (adj.sum(dim=1) == 0).any():\n",
    "            raise ValueError(\"Not all nodes are connected to at least one other node.\")\n",
    "\n",
    "        for node_idx in range(num_nodes):\n",
    "            # Get the neighbors of the current node\n",
    "            neighbors = adj[node_idx].nonzero().squeeze(dim = 1)  # Shape: (num_neighbors)\n",
    "            # Initialize attention scores for the current node\n",
    "            attention_scores = torch.zeros(len(neighbors))\n",
    "\n",
    "            for i, neighbor_idx in enumerate(neighbors):\n",
    "                # Create queries and keys for attention for the current node and its neighbors\n",
    "                h_v = x_transformed[node_idx]  \n",
    "                h_u = x_transformed[neighbor_idx] \n",
    "\n",
    "                # Calculate attention score for the current neighbor\n",
    "                concat = torch.cat([h_v.unsqueeze(0), h_u.unsqueeze(0)], dim=-1)  # Shape: (1, 2 * out_features)\n",
    "                # Attention score for the current neighbor\n",
    "                attention_scores[i] = F.leaky_relu(self.attention_s_t(concat))\n",
    "\n",
    "            # Set attention scores to -inf for nodes not connected to the current node\n",
    "            attention_scores[adj[node_idx, neighbors] == 0] = float(\"-inf\")\n",
    "\n",
    "            # Normalize attention scores a_uv = e_uv / sum(e_uv)\n",
    "            attention_weights = F.softmax(attention_scores, dim=0)\n",
    "\n",
    "            # Aggregate neighbor informationm, default is sum as in the statements\n",
    "            if self.aggregation == \"sum\":\n",
    "                aggregated = torch.sum(attention_weights.unsqueeze(1) * x_transformed[neighbors], dim=0)\n",
    "            elif self.aggregation == \"mean\":\n",
    "                aggregated = torch.mean(attention_weights.unsqueeze(1) * x_transformed[neighbors], dim=0)\n",
    "            elif self.aggregation == \"max\":\n",
    "                aggregated, _ = torch.max(attention_weights.unsqueeze(1) * x_transformed[neighbors], dim=0)\n",
    "            else:\n",
    "                raise ValueError(\"Aggregation type not supported.\")\n",
    "\n",
    "            # Apply activation function\n",
    "            if self.activation is not None:\n",
    "                aggregated = self.activation(aggregated)\n",
    "\n",
    "            # Update the output features\n",
    "            output_features[node_idx] = aggregated\n",
    "\n",
    "        return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented the 3 different methods, one considering only the nodes and two considering the nodes and the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : only the node features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of two parts: a part with a convolution part with a pooling layer and a neural network. The convolution part updates the features, the features are pooled by choosing either a max pooling or a mean pooling. Then the features are sent to the neural network to predict the class of the molecule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the data + explore it using pandas (node only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_index</th>\n",
       "      <th>node_feat</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>y</th>\n",
       "      <th>num_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 6,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          edge_index   \n",
       "0  [[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6,...  \\\n",
       "1  [[0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,...   \n",
       "2  [[0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 6,...   \n",
       "3  [[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6,...   \n",
       "4  [[0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5,...   \n",
       "\n",
       "                                           node_feat   \n",
       "0  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \\\n",
       "1  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....   \n",
       "2  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....   \n",
       "3  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....   \n",
       "4  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....   \n",
       "\n",
       "                                           edge_attr    y  num_nodes  \n",
       "0  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [1]         17  \n",
       "1  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [0]         13  \n",
       "2  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [0]         13  \n",
       "3  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [1]         19  \n",
       "4  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [0]         11  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data into a DataFrame\n",
    "df = pd.read_json('full.jsonl', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 11, 11, 12, 12, 12, 13, 13, 14, 14, 14, 15, 16], [1, 5, 0, 2, 1, 3, 2, 4, 9, 3, 5, 6, 0, 4, 4, 7, 6, 8, 7, 9, 13, 3, 8, 10, 9, 11, 10, 12, 11, 13, 14, 8, 12, 12, 15, 16, 14, 14]]\n"
     ]
    }
   ],
   "source": [
    "# Show what is the egde_index\n",
    "print(df['edge_index'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of y=1 over all y: 0.6648936170212766\n"
     ]
    }
   ],
   "source": [
    "# Find the ratio of y=1 over all y\n",
    "\n",
    "# Calculate the count of y=1\n",
    "count_y_equals_1 = df[df['y'].apply(lambda x: 1 in x)]['y'].count()\n",
    "\n",
    "# Calculate the total count of all y values\n",
    "total_count_y = df['y'].count()\n",
    "\n",
    "ratio_y_equals_1 = count_y_equals_1 / total_count_y\n",
    "print(\"Ratio of y=1 over all y:\", ratio_y_equals_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 66% of all y = 1, so we have a \"balanced\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the size of the totatl data\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take back the information from the data description\n",
    "\n",
    "Each row of a given file is a graph, with:\n",
    "\n",
    "- node_feat (list: #nodes x #node-features): nodes\n",
    "- edge_index (list: 2 x #edges): pairs of nodes constituting edges\n",
    "- edge_attr (list: #edges x #edge-features): for the aforementioned edges, contains their features (edge features)\n",
    "- y (list: 1 x #labels): contains the number of labels available to predict (here 1, equal to zero or one)\n",
    "- num_nodes (int): number of nodes of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace edge_index by an adjency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that transforms the edge_index to adjacency matrix\n",
    "def edge_index_to_adjacency_matrix(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Convert edge index representation to an adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "        edge_index (numpy matrix): pairs of nodes constituting edges\n",
    "        num_nodes (int): number of nodes in the graph\n",
    "\n",
    "    Returns:\n",
    "        numpy matrix: adjency matrix of the graph\n",
    "    \"\"\"\n",
    "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "    for i in range(0, len(edge_index), 2):\n",
    "        src, tgt = edge_index[i], edge_index[i+1]\n",
    "        adj_matrix[src, tgt] = 1\n",
    "        adj_matrix[tgt, src] = 1  # Assuming the graph is undirected\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace edge_index with adjacency matrices\n",
    "df['edge_index'] = df.apply(lambda row: edge_index_to_adjacency_matrix(row['edge_index'], row['num_nodes']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the data into train, validation, test set with 70%, 15%, 15% of the data (node only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 131\n",
      "Testing set size: 28\n",
      "Validation set size: 29\n"
     ]
    }
   ],
   "source": [
    "# Define the desired percentages for training, testing, and validation\n",
    "train_percent = 0.7\n",
    "test_percent = 0.15\n",
    "validation_percent = 0.15\n",
    "\n",
    "# Shuffle the DataFrame randomly\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# define the number of rows in each set. We set the number of rows to be integers, we set first the row for the training, test set, and then rest for the validation set\n",
    "total_rows = len(df)\n",
    "train_size = int(train_percent * total_rows)\n",
    "test_size = int(test_percent * total_rows)\n",
    "\n",
    "# Create training, testing, and validation sets\n",
    "train_set = df.iloc[:train_size]\n",
    "test_set = df.iloc[train_size:(train_size + test_size)]\n",
    "validation_set = df.iloc[(train_size + test_size):]\n",
    "\n",
    "# Print the number of rows in each set\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Testing set size: {len(test_set)}\")\n",
    "print(f\"Validation set size: {len(validation_set)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform in pytorch and use a dataloader (node only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initialize the GraphDataset.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): A DataFrame containing graph data, where each row\n",
    "                represents a graph\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of graphs in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of graphs.\n",
    "        \"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    #get the value of each column\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get graph data at a specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the graph in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Edge indices for the graph.\n",
    "            torch.Tensor: Node features for the graph.\n",
    "            torch.Tensor: Edge attributes for the graph.\n",
    "            torch.Tensor: Labels for the graph.\n",
    "            int: Number of nodes in the graph.\n",
    "        \"\"\"\n",
    "        edge_index = torch.tensor(self.dataframe.iloc[idx]['edge_index'])\n",
    "        node_feat = torch.tensor(self.dataframe.iloc[idx]['node_feat'])\n",
    "        edge_attr = torch.tensor(self.dataframe.iloc[idx]['edge_attr'])\n",
    "        y = torch.tensor(self.dataframe.iloc[idx]['y'])\n",
    "        num_nodes = self.dataframe.iloc[idx]['num_nodes']\n",
    "\n",
    "        return edge_index, node_feat, edge_attr, y, num_nodes\n",
    "    \n",
    "def custom_collate(batch):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom collate function for batching graph data. Given a batch of graph samples, \n",
    "    it separates and returns five lists.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of graph data, where each item is a tuple containing\n",
    "                      (edge_index, node_feat, edge_attr, y, num_nodes) for a graph.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing five lists, each containing the collected data from the batch:\n",
    "            - edge_index: List of edge indices for each graph.\n",
    "            - node_feat: List of node features for each graph.\n",
    "            - edge_attr: List of edge attributes for each graph.\n",
    "            - y: List of target labels for each graph.\n",
    "            - num_nodes: List of the number of nodes for each graph.\n",
    "    \"\"\"\n",
    "    edge_index = [item[0] for item in batch]\n",
    "    node_feat = [item[1] for item in batch]\n",
    "    edge_attr = [item[2] for item in batch]\n",
    "    y = [item[3] for item in batch]\n",
    "    num_nodes = [item[4] for item in batch]\n",
    "    \n",
    "    return edge_index, node_feat, edge_attr, y, num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from a pandas dataframe to a sth that pytorch can use\n",
    "train_dataset = GraphDataset(train_set)\n",
    "test_dataset = GraphDataset(test_set)\n",
    "validation_dataset = GraphDataset(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "Batch Edge Index: tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
      "       dtype=torch.float64)\n",
      "Batch Node Features: tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.]])\n",
      "Batch Edge Attributes: tensor([[0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n",
      "Batch Labels: tensor([0])\n",
      "Batch Number of Nodes: 17\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader with custom collate function\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "# Inspect the first batch\n",
    "for i, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes) in enumerate(train_dataloader):\n",
    "    if i == 0:\n",
    "        print(\"Batch:\", i+1)\n",
    "        print(\"Batch Edge Index:\", batch_edge_index[0])\n",
    "        print(\"Batch Node Features:\", batch_node_feat[0])\n",
    "        print(\"Batch Edge Attributes:\", batch_edge_attr[0])\n",
    "        print(\"Batch Labels:\", batch_y[0])\n",
    "        print(\"Batch Number of Nodes:\", batch_num_nodes[0])\n",
    "        \n",
    "        # Stop after the first batch for demonstration\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to determine the number of features in the graph and see if there is the same number of features in all graphs. For that we check that the min number of features is the same as the max number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of features: 7\n",
      "Min number of features: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "max_features = 0\n",
    "min_features = 10000000000\n",
    "\n",
    "# Loop through each row in the 'node_feat' column\n",
    "for row in df['node_feat']:\n",
    "    # we suppose that inside the graph, all the nodes have the same number of features, we take only the number of features of the first node\n",
    "    num_features = len(row[0]) if row else 0 \n",
    "    # Update max and min number of features\n",
    "    max_features = max(max_features, num_features)\n",
    "    min_features = min(min_features, num_features)\n",
    "\n",
    "# we set the number of features to be the max number of features\n",
    "num_features_df = max_features\n",
    "\n",
    "print(\"Max number of features:\", max_features)\n",
    "print(\"Min number of features:\", min_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data type that can be send to pytorch and be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define graph neural network that will predict the mutagenicity\n",
    "\n",
    "We define a GNN that will be trained which correspond to this part in the picture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphNetwork(nn.Module):\n",
    "    def __init__(self, num_in_features, numb_out_features, conv_layers, aggregation, number_of_hidden_neurons = []):\n",
    "        \"\"\"\n",
    "        Initialize the custom graph neural network.\n",
    "\n",
    "        Args:\n",
    "            num_in_features (int): Number of input node features.\n",
    "            num_out_features (int): Number of output features for each convolutional layer.\n",
    "            conv_layers (nn.ModuleList): List of graph convolutional layer instances.\n",
    "            aggregation (nn.Module): Instance of pooling layer.\n",
    "            number_of_hidden_neurons (list): List of hidden layer sizes for fully connected layers.\n",
    "        \"\"\"\n",
    "     \n",
    "        super(CustomGraphNetwork, self).__init__()\n",
    "        # Infeatures: number of input features per node\n",
    "        self.num_in_features = num_in_features\n",
    "        # Outfeatures: number of output features per node\n",
    "        self.num_out_features = numb_out_features\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        \n",
    "        # Aggregation function: which is applied to aggregate into a shape 1, out_features        \n",
    "        self.aggregation = aggregation\n",
    "        \n",
    "        # list of hidden layer sizes for fully connected layers\n",
    "        self.number_of_hidden_neurons = number_of_hidden_neurons\n",
    "        self.fully_connected = nn.ModuleList()\n",
    "        in_features_of_fully_connected = self.num_out_features\n",
    "        # build of the fully connected layers\n",
    "        for i in number_of_hidden_neurons:\n",
    "            self.fully_connected.append(nn.Linear(in_features_of_fully_connected, i))\n",
    "            in_features_of_fully_connected = i\n",
    "        # add the last fully connected layer\n",
    "        self.fully_connected.append(nn.Linear(in_features_of_fully_connected, 2))    \n",
    "        \n",
    "    def forward(self, x,adj):\n",
    "        \"\"\"\n",
    "        Forward pass through the custom graph neural network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features, shape (num_nodes, num_in_features).\n",
    "            adj (Tensor): Adjacency matrix, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: with the probality of each graph to be in the class 0 or 1\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # Pass input through each graph convolutional layer        \n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x, adj)  # Pass adjacency matrix to the convolutional layer\n",
    "        graph_output = x\n",
    "\n",
    "        # Apply global pooling along the node dimension (dimension 1)\n",
    "        if self.aggregation == \"mean\" :\n",
    "            graph_output = torch.mean(graph_output, dim=0, keepdim=False)\n",
    "        elif self.aggregation == \"max\" :\n",
    "            graph_output, _ = torch.max(graph_output, dim=0, keepdim=False)\n",
    "        else :\n",
    "            raise ValueError(\"Aggregation type not supported.\")\n",
    "              \n",
    "\n",
    "        # Apply the fully connected layers        \n",
    "        for fc_layer in self.fully_connected:\n",
    "            graph_output = fc_layer(graph_output)\n",
    "\n",
    "        # Apply softmax activation\n",
    "        softmax_output = torch.softmax(graph_output, dim=0) \n",
    "\n",
    "        return softmax_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with only the node features, create a function that will train the model and return the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a GNN using a training loop.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be trained.\n",
    "    - optimizer: The optimizer for updating model parameters.\n",
    "    - criterion: The loss function used for optimization.\n",
    "    - train_dataloader: DataLoader providing training data.\n",
    "    - num_epochs: The number of training epochs.\n",
    "    - learning_rate: The learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    - trained_model: The trained machine learning model.\n",
    "    - training_accuracy: The overall training accuracy across all epochs.\n",
    "    - losses: A list of losses during training.\n",
    "    \"\"\"\n",
    "    # Initialize variables for training accuracy and losses    \n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "    losses = []\n",
    "    \n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    \n",
    "    # Training loop, epoch by epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize variables for training accuracy and losses for each epoch\n",
    "        sub_accuracy_list = []\n",
    "        correct_train_predictions_epoch = 0\n",
    "        total_train_samples_epoch = 0      \n",
    "\n",
    "        # Loop through each batch in the training dataloader\n",
    "        for idx, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes) in enumerate(train_dataloader):\n",
    "            \n",
    "            # loop through each graph in the batch\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                \n",
    "                # Extract features, adjency matrix and labels for the current graph\n",
    "                node_features = batch_node_feat[i].float()\n",
    "                adj_matrix = batch_edge_index[i].float()\n",
    "                ground_truth_labels = batch_y[i].clone()\n",
    "                \n",
    "                # Convert labels to one-hot labels\n",
    "                one_hot_labels = torch.zeros((ground_truth_labels.size(0), 2))\n",
    "                one_hot_labels.scatter_(1, ground_truth_labels.view(-1, 1), 1).squeeze_()\n",
    "                \n",
    "                # Forward pass, make a prediction\n",
    "                output = model(node_features, adj_matrix)\n",
    "\n",
    "                # Calculate the predicted class based on probability of the output\n",
    "                predicted_class = (output > 0.5).long()\n",
    "                # condition : if the prediction == ground truth -> add 1 to the correct_train_predictions\n",
    "                condition = (predicted_class == one_hot_labels).all().item()\n",
    "                if condition:\n",
    "                    correct_train_predictions += 1\n",
    "                    correct_train_predictions_epoch = correct_train_predictions_epoch + 1\n",
    "                #Calculate the total number of samples\n",
    "                total_train_samples += 1\n",
    "                total_train_samples_epoch = total_train_samples_epoch + 1  \n",
    "                \n",
    "                # transform the output to probabilities for loss calculation\n",
    "                output_probabilities = output[1]  # Select the second element (probability for class 1)\n",
    "                ground_truth_labels =  ground_truth_labels.squeeze()\n",
    "                output_logits = torch.log(output_probabilities / (1 - output_probabilities))# Calculate the logits for the loss function\n",
    "                # Calculate the loss\n",
    "                loss = criterion(output_logits, ground_truth_labels.float())\n",
    "                losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                # optimizer.step()\n",
    "                # Optimizer step does not work, Update weights using gradient descent we do this manually because I do not understand why optimizer.step() does not work :@              \n",
    "                with torch.no_grad(): \n",
    "                    for name, param in model.named_parameters():\n",
    "                        param -= learning_rate * param.grad\n",
    "                            \n",
    "        # Calculate training accuracy\n",
    "        training_accuracy = (correct_train_predictions_epoch / total_train_samples_epoch) * 100\n",
    "        sub_accuracy_list.append(training_accuracy)\n",
    "        \n",
    "        # Print training accuracy for each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Accuracy: {training_accuracy:.2f}%\")\n",
    "        \n",
    "        #Calculate the mean of the accuracy for all the epochs\n",
    "        training_accuracy_final = (correct_train_predictions / total_train_samples) * 100\n",
    "\n",
    "    return model, training_accuracy_final, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate with only the node features, create a function that will train the model and return the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validation_dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Validate a GNN using a validation loop.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be trained.\n",
    "    - validation_dataloader: DataLoader providing training data.\n",
    "    - criterion: The loss function used for optimization.\n",
    "\n",
    "    Returns:\n",
    "    - average_validation_loss: the average validation loss\n",
    "    - validation_accuracy: The overall validation accuracy across all epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to evaluation mode (no gradient updates)\n",
    "    model.eval()  \n",
    "    \n",
    "    # Initialize variables for validation accuracy and loss\n",
    "    # We have for each two numbers so we can calculate the mean of the accuracy and loss\n",
    "    total_validation_loss = 0.0\n",
    "    validation_batches = 0\n",
    "\n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for idx, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes) in enumerate(validation_dataloader):\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                # Extract features, adjency matrix and labels for the current graph\n",
    "                node_features = batch_node_feat[i].float()\n",
    "                adj_matrix = batch_edge_index[i].float()\n",
    "                ground_truth_labels = batch_y[i].clone().detach()\n",
    "\n",
    "                # Convert labels to one-hot labels\n",
    "                one_hot_labels = torch.zeros((ground_truth_labels.size(0), 2))\n",
    "                one_hot_labels.scatter_(1, ground_truth_labels.view(-1, 1), 1).squeeze_()\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(node_features, adj_matrix)\n",
    "\n",
    "                # Calculate the predicted class based on probability of the output\n",
    "                predicted_class = (output > 0.5).long()\n",
    "                # condition : if the prediction == ground truth -> add 1 to the correct_train_predictions\n",
    "                condition = (predicted_class == one_hot_labels).all().item()\n",
    "                if condition:\n",
    "                    correct_train_predictions += 1\n",
    "                total_train_samples += 1\n",
    "\n",
    "                # Compute the validation loss\n",
    "                validation_loss = criterion(output, one_hot_labels)\n",
    "                \n",
    "                # add the validation loss to the total validation loss\n",
    "                total_validation_loss += validation_loss.item()\n",
    "                validation_batches += 1\n",
    "\n",
    "    # Calculate the average validation loss\n",
    "    average_validation_loss = total_validation_loss / validation_batches\n",
    "    # Calculate validation accuracy\n",
    "    validation_accuracy = (correct_train_predictions / total_train_samples) * 100\n",
    "\n",
    "    return average_validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare informations for multiple training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the number of features that we want to put in the model and the number of features that is going out of the model\n",
    "# We consider that they remain the same\n",
    "in_features= max_features\n",
    "out_features = max_features #can be different\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the learning rate, number of epochs\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.18%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.94%\n",
      "Epoch [6/20], Training Accuracy: 66.41%\n",
      "Epoch [7/20], Training Accuracy: 67.94%\n",
      "Epoch [8/20], Training Accuracy: 73.28%\n",
      "Epoch [9/20], Training Accuracy: 72.52%\n",
      "Epoch [10/20], Training Accuracy: 72.52%\n",
      "Epoch [11/20], Training Accuracy: 70.99%\n",
      "Epoch [12/20], Training Accuracy: 73.28%\n",
      "Epoch [13/20], Training Accuracy: 68.70%\n",
      "Epoch [14/20], Training Accuracy: 72.52%\n",
      "Epoch [15/20], Training Accuracy: 73.28%\n",
      "Epoch [16/20], Training Accuracy: 73.28%\n",
      "Epoch [17/20], Training Accuracy: 74.05%\n",
      "Epoch [18/20], Training Accuracy: 74.05%\n",
      "Epoch [19/20], Training Accuracy: 70.99%\n",
      "Epoch [20/20], Training Accuracy: 70.99%\n",
      "Training loss: 0.9355\n",
      "Training accuracy final: 70.57%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers, we can stack mutiple layers in a list\n",
    "conv_layers = [GraphConv(in_features, out_features, \"max\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork(in_features, out_features, conv_layers, \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model with GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6624461474089787\n",
      "Validation Accuracy: 72.41%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat but using GraphSAGE instead of GraphConv for the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.18%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 67.18%\n",
      "Epoch [8/20], Training Accuracy: 67.18%\n",
      "Epoch [9/20], Training Accuracy: 65.65%\n",
      "Epoch [10/20], Training Accuracy: 70.99%\n",
      "Epoch [11/20], Training Accuracy: 67.18%\n",
      "Epoch [12/20], Training Accuracy: 67.18%\n",
      "Epoch [13/20], Training Accuracy: 70.99%\n",
      "Epoch [14/20], Training Accuracy: 70.23%\n",
      "Epoch [15/20], Training Accuracy: 70.99%\n",
      "Epoch [16/20], Training Accuracy: 70.99%\n",
      "Epoch [17/20], Training Accuracy: 70.99%\n",
      "Epoch [18/20], Training Accuracy: 70.99%\n",
      "Epoch [19/20], Training Accuracy: 71.76%\n",
      "Epoch [20/20], Training Accuracy: 70.99%\n",
      "Training loss: 0.2929\n",
      "Training accuracy final: 68.82%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers, we can stack mutiple layers in a list\n",
    "conv_layers = [GraphSAGEConv(in_features, out_features, \"max\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork(in_features, out_features, conv_layers, \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model with GraphSAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6963810427435513\n",
      "Validation Accuracy: 68.97%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat but using Attention instead of GraphConv for the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 36.64%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 67.18%\n",
      "Epoch [8/20], Training Accuracy: 67.18%\n",
      "Epoch [9/20], Training Accuracy: 67.18%\n",
      "Epoch [10/20], Training Accuracy: 67.18%\n",
      "Epoch [11/20], Training Accuracy: 67.18%\n",
      "Epoch [12/20], Training Accuracy: 67.18%\n",
      "Epoch [13/20], Training Accuracy: 67.18%\n",
      "Epoch [14/20], Training Accuracy: 67.18%\n",
      "Epoch [15/20], Training Accuracy: 67.18%\n",
      "Epoch [16/20], Training Accuracy: 67.18%\n",
      "Epoch [17/20], Training Accuracy: 67.18%\n",
      "Epoch [18/20], Training Accuracy: 67.18%\n",
      "Epoch [19/20], Training Accuracy: 67.18%\n",
      "Epoch [20/20], Training Accuracy: 67.18%\n",
      "Training loss: 0.4669\n",
      "Training accuracy final: 65.65%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers, we can stack mutiple layers in a list\n",
    "conv_layers = [AttentionConv(in_features, out_features, \"sum\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork(in_features, out_features, conv_layers, \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7064056190951117\n",
      "Validation Accuracy: 65.52%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "# Adding edge features solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first idea was to concatenate into the features of the nodes, a combination of the features of the edges. These features of the edges are the new features of the nodes. This offers the possibility to not change the code related to the convolution, the neural network, or the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a function in panda that will allow us to add the features of the edges to the features of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_node_with_edge_features(df):\n",
    "    \"\"\"\n",
    "    Concatenates the node features with the mean of the edge features for each node.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'node_feat', 'edge_index', 'edge_attr', etc.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with updated 'node_feat'.\n",
    "    \"\"\"\n",
    "    #iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        num_nodes = row['num_nodes']\n",
    "        num_edge_features = len(row['edge_attr'][0])\n",
    "\n",
    "        # Initialize lists to store the sum of edge attributes and number of edges for each node\n",
    "        summed_edge_attr = [[0.0] * num_edge_features for _ in range(num_nodes)]\n",
    "        edge_count = [0] * num_nodes\n",
    "\n",
    "        # Get source and destination nodes from edge_index\n",
    "        src_nodes, dst_nodes = row['edge_index']\n",
    "\n",
    "        # Sum edge attributes for each node and count the number of edges\n",
    "        for i in range(len(src_nodes)):\n",
    "            # Get source and destination nodes for the current edge\n",
    "            src = src_nodes[i]\n",
    "            dst = dst_nodes[i]\n",
    "\n",
    "            # Add edge attributes to the sum for the source and destination nodes\n",
    "            for j in range(num_edge_features):\n",
    "                summed_edge_attr[src][j] += row['edge_attr'][i][j]\n",
    "                summed_edge_attr[dst][j] += row['edge_attr'][i][j]\n",
    "\n",
    "            # Increment the edge count for the source and destination nodes\n",
    "            edge_count[src] += 1\n",
    "            edge_count[dst] += 1\n",
    "\n",
    "        # Calculate the mean edge attributes for each node\n",
    "        mean_edge_attr = [\n",
    "            [s / max(c, 1) for s, c in zip(sum_attr, edge_count)]\n",
    "            for sum_attr in summed_edge_attr\n",
    "        ]\n",
    "\n",
    "        # Concatenate node features with mean edge attributes\n",
    "        new_node_feat = [feat + mean for feat, mean in zip(row['node_feat'], mean_edge_attr)]\n",
    "\n",
    "        # Update the DataFrame\n",
    "        df.at[index, 'node_feat'] = new_node_feat\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reload the data as we have previously modified a lot of things in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we delete the edge_attr column, we need to re-load the data\n",
    "df = pd.read_json('full.jsonl', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function to add the features of the edges to the features of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate node features with mean edge attributes\n",
    "df = concatenate_node_with_edge_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the number of edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(df['edge_attr'][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we check that the number of features in the number of features is the sum (11) of the number of features before (7) plus the number of features of the edge_attr (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Node feature must now be 7+4 =11\n",
    "print(len(df['node_feat'][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we prefer to use an adjency matrix instead of edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace edge_index with adjacency matrices\n",
    "df['edge_index'] = df.apply(lambda row: edge_index_to_adjacency_matrix(row['edge_index'], row['num_nodes']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the data into train, validation, test set with 70%, 15%, 15% of the data (edges 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 131\n",
      "Testing set size: 28\n",
      "Validation set size: 29\n"
     ]
    }
   ],
   "source": [
    "# Define the desired percentages for training, testing, and validation\n",
    "train_percent = 0.7\n",
    "test_percent = 0.15\n",
    "validation_percent = 0.15\n",
    "\n",
    "# Shuffle the DataFrame randomly\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Calculate the number of rows for each split\n",
    "total_rows = len(df)\n",
    "train_size = int(train_percent * total_rows)\n",
    "test_size = int(test_percent * total_rows)\n",
    "\n",
    "# Create training, testing, and validation sets\n",
    "train_set = df.iloc[:train_size]\n",
    "test_set = df.iloc[train_size:(train_size + test_size)]\n",
    "validation_set = df.iloc[(train_size + test_size):]\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Testing set size: {len(test_set)}\")\n",
    "print(f\"Validation set size: {len(validation_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform in pytorch and use a dataloader (edges 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "train_dataset = GraphDataset(train_set)\n",
    "test_dataset = GraphDataset(test_set)\n",
    "validation_dataset = GraphDataset(validation_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "Batch Edge Index: torch.Size([20, 20])\n",
      "Batch Node Features: torch.Size([20, 11])\n",
      "Batch Edge Attributes: torch.Size([44, 4])\n",
      "Batch Labels: torch.Size([1])\n",
      "Batch Number of Nodes: 20\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader with custom collate function\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "# Inspect the first batch, but only the shapes this time\n",
    "for i, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes) in enumerate(train_dataloader):\n",
    "    if i == 0:\n",
    "        print(\"Batch:\", i+1)\n",
    "        print(\"Batch Edge Index:\", batch_edge_index[0].shape)\n",
    "        print(\"Batch Node Features:\", batch_node_feat[0].shape)\n",
    "        print(\"Batch Edge Attributes:\", batch_edge_attr[0].shape)\n",
    "        print(\"Batch Labels:\", batch_y[0].shape)\n",
    "        print(\"Batch Number of Nodes:\", batch_num_nodes[0])\n",
    "        \n",
    "        # Stop after the first batch for demonstration\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to determine the number of features in the graph and see if there is the same number of features in all graphs. For that we check that the min number of features is the same as the max number of features. (with edges) (edge 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of features: 11\n",
      "Min number of features: 11\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "max_features = 0\n",
    "min_features = 10000000000\n",
    "\n",
    "# Loop through each row in the 'node_feat' column\n",
    "for row in df['node_feat']:\n",
    "    # we suppose that inside the graph, all the nodes have the same number of features, we take only the number of features of the first node\n",
    "    num_features = len(row[0]) if row else 0 \n",
    "    # Update max and min number of features\n",
    "    max_features = max(max_features, num_features)\n",
    "    min_features = min(min_features, num_features)\n",
    "\n",
    "# we set the number of features to be the max number of features\n",
    "num_features_df = max_features\n",
    "\n",
    "print(\"Max number of features:\", max_features)\n",
    "print(\"Min number of features:\", min_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data type that can be send to pytorch and be used for training.\n",
    "\n",
    "prepare informations for multiple training and validation\n",
    "we keep the same learning rate and the same number of epochs as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the number of features that we want to put in the model and the number of features that is going out of the model\n",
    "# We consider that they remain the same\n",
    "in_features= max_features\n",
    "out_features = max_features #can be different\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.18%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 70.99%\n",
      "Epoch [5/20], Training Accuracy: 74.05%\n",
      "Epoch [6/20], Training Accuracy: 76.34%\n",
      "Epoch [7/20], Training Accuracy: 78.63%\n",
      "Epoch [8/20], Training Accuracy: 81.68%\n",
      "Epoch [9/20], Training Accuracy: 80.92%\n",
      "Epoch [10/20], Training Accuracy: 82.44%\n",
      "Epoch [11/20], Training Accuracy: 83.97%\n",
      "Epoch [12/20], Training Accuracy: 86.26%\n",
      "Epoch [13/20], Training Accuracy: 83.97%\n",
      "Epoch [14/20], Training Accuracy: 72.52%\n",
      "Epoch [15/20], Training Accuracy: 76.34%\n",
      "Epoch [16/20], Training Accuracy: 76.34%\n",
      "Epoch [17/20], Training Accuracy: 82.44%\n",
      "Epoch [18/20], Training Accuracy: 83.21%\n",
      "Epoch [19/20], Training Accuracy: 85.50%\n",
      "Epoch [20/20], Training Accuracy: 81.68%\n",
      "Training loss: 0.0412\n",
      "Training accuracy final: 77.94%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers, we can stack mutiple layers in a list\n",
    "conv_layers = [GraphConv(in_features, out_features, \"max\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork(in_features, out_features, conv_layers, \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6228203382985346\n",
      "Validation Accuracy: 79.31%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat but using GraphSAGE instead of GraphConv for the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 68.70%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 68.70%\n",
      "Epoch [8/20], Training Accuracy: 68.70%\n",
      "Epoch [9/20], Training Accuracy: 73.28%\n",
      "Epoch [10/20], Training Accuracy: 73.28%\n",
      "Epoch [11/20], Training Accuracy: 70.23%\n",
      "Epoch [12/20], Training Accuracy: 77.10%\n",
      "Epoch [13/20], Training Accuracy: 77.10%\n",
      "Epoch [14/20], Training Accuracy: 72.52%\n",
      "Epoch [15/20], Training Accuracy: 78.63%\n",
      "Epoch [16/20], Training Accuracy: 79.39%\n",
      "Epoch [17/20], Training Accuracy: 76.34%\n",
      "Epoch [18/20], Training Accuracy: 78.63%\n",
      "Epoch [19/20], Training Accuracy: 80.92%\n",
      "Epoch [20/20], Training Accuracy: 80.92%\n",
      "Training loss: 0.0777\n",
      "Training accuracy final: 73.02%\n"
     ]
    }
   ],
   "source": [
    "conv_layers = [GraphSAGEConv(in_features, out_features, \"max\", activation=nn.ReLU())]\n",
    "model = CustomGraphNetwork(in_features, out_features, conv_layers, \"max\",[])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, training_accuracy_final, losses = train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6462649723579144\n",
      "Validation Accuracy: 75.86%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat but using Attention instead of GraphConv for the convolutional layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.94%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 67.18%\n",
      "Epoch [8/20], Training Accuracy: 67.18%\n",
      "Epoch [9/20], Training Accuracy: 67.18%\n",
      "Epoch [10/20], Training Accuracy: 67.18%\n",
      "Epoch [11/20], Training Accuracy: 67.18%\n",
      "Epoch [12/20], Training Accuracy: 67.18%\n",
      "Epoch [13/20], Training Accuracy: 67.18%\n",
      "Epoch [14/20], Training Accuracy: 67.18%\n",
      "Epoch [15/20], Training Accuracy: 67.18%\n",
      "Epoch [16/20], Training Accuracy: 67.18%\n",
      "Epoch [17/20], Training Accuracy: 67.18%\n",
      "Epoch [18/20], Training Accuracy: 67.18%\n",
      "Epoch [19/20], Training Accuracy: 67.18%\n",
      "Epoch [20/20], Training Accuracy: 67.18%\n",
      "Training loss: 1.4712\n",
      "Training accuracy final: 67.21%\n"
     ]
    }
   ],
   "source": [
    "conv_layers = [AttentionConv(in_features, out_features, \"sum\", activation=nn.ReLU())]\n",
    "model = CustomGraphNetwork(in_features, out_features, conv_layers, \"max\",[])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, training_accuracy_final, losses = train_model(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6893690865615318\n",
      "Validation Accuracy: 65.52%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "# Adding edge features solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second option was to reverse the idea of node and edges. By considering that each edge is connected to another edge by a node. By using this idea, we can convolve edge features as we did for node features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using this idea, we can convolve edge features as we did for node features and combine the edge features pooling with the node features pooling together in the neural network. This offers the advantage that only the CustomGraphNetwork classes and the train loop need to be modified. Moreover, it gives the advantage that the edge features are not aggregated according to the nodes they belong to, but aggregates based on an adjacency matrix built for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reload the data as we have previously modified a lot of things in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('full.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new column to the dataframe that contains the number of edges of each graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_edges'] = df['edge_attr'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that will create an adjency matrix for the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_adj_matrix_for_df(df):\n",
    "    \"\"\"\n",
    "    Build edge adjacency matrices for each row in a DataFrame and add them as a new column.\n",
    "    \n",
    "    :param df: DataFrame containing graph info in each row.\n",
    "    :return: A DataFrame with a new column \"edge_adj\" that contains the edge adjacency matrices.\n",
    "    \"\"\"\n",
    "    edge_adj_matrices = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        edge_index = row['edge_index']\n",
    "        num_edges = row['num_edges']\n",
    "        \n",
    "        edge_adj_matrix = np.zeros((num_edges, num_edges), dtype=int)\n",
    "        \n",
    "        for i in range(num_edges):\n",
    "            a1, b1 = edge_index[0][i], edge_index[1][i]\n",
    "            for j in range(num_edges):\n",
    "                if i != j:\n",
    "                    a2, b2 = edge_index[0][j], edge_index[1][j]\n",
    "                    \n",
    "                    if a1 == b2 or b1 == a2 or a1 == a2 or b1 == b2 :\n",
    "                        edge_adj_matrix[i][j] = 1\n",
    "        \n",
    "        edge_adj_matrices.append(edge_adj_matrix)\n",
    "        \n",
    "    df['edge_adj'] = edge_adj_matrices\n",
    "    return df\n",
    "\n",
    "df = build_edge_adj_matrix_for_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we incorporated the edge adjency matrix in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_index</th>\n",
       "      <th>node_feat</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>y</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>edge_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>[[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>[[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          edge_index   \n",
       "0  [[0, 0, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 6,...  \\\n",
       "1  [[0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,...   \n",
       "\n",
       "                                           node_feat   \n",
       "0  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \\\n",
       "1  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....   \n",
       "\n",
       "                                           edge_attr    y  num_nodes   \n",
       "0  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [1]         17  \\\n",
       "1  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [0]         13   \n",
       "\n",
       "   num_edges                                           edge_adj  \n",
       "0         38  [[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...  \n",
       "1         28  [[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we prefer to use an node adjency matrix instead of edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edge_index'] = df.apply(lambda row: edge_index_to_adjacency_matrix(row['edge_index'], row['num_nodes']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we pick a look at our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_index</th>\n",
       "      <th>node_feat</th>\n",
       "      <th>edge_attr</th>\n",
       "      <th>y</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>edge_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>[[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>[[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          edge_index   \n",
       "0  [[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \\\n",
       "1  [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                           node_feat   \n",
       "0  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....  \\\n",
       "1  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0....   \n",
       "\n",
       "                                           edge_attr    y  num_nodes   \n",
       "0  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [1]         17  \\\n",
       "1  [[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [...  [0]         13   \n",
       "\n",
       "   num_edges                                           edge_adj  \n",
       "0         38  [[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...  \n",
       "1         28  [[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the data into train, validation, test set with 70%, 15%, 15% of the data (edges 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 131\n",
      "Testing set size: 28\n",
      "Validation set size: 29\n"
     ]
    }
   ],
   "source": [
    "# Define the desired percentages for training, testing, and validation\n",
    "train_percent = 0.7\n",
    "test_percent = 0.15\n",
    "validation_percent = 0.15\n",
    "\n",
    "# Shuffle the DataFrame randomly\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Calculate the number of rows for each split\n",
    "total_rows = len(df)\n",
    "train_size = int(train_percent * total_rows)\n",
    "test_size = int(test_percent * total_rows)\n",
    "\n",
    "# Create training, testing, and validation sets\n",
    "train_set = df.iloc[:train_size]\n",
    "test_set = df.iloc[train_size:(train_size + test_size)]\n",
    "validation_set = df.iloc[(train_size + test_size):]\n",
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Testing set size: {len(test_set)}\")\n",
    "print(f\"Validation set size: {len(validation_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform in pytorch and use a dataloader (edges 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from now on, we cannot use the origninal function because of these new edge features. However, we can use the same function as before but by adding the edge adjency matrix as a parameter. They are very similar to the previous function. Therefore they are written as \"name_of_the_function_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset_2(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initialize the GraphDataset.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): A DataFrame containing graph data, where each row\n",
    "                represents a graph\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of graphs in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of graphs.\n",
    "        \"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    #get the value of each column\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get graph data at a specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the graph in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Edge indices for the graph.\n",
    "            torch.Tensor: Node features for the graph.\n",
    "            torch.Tensor: Edge attributes for the graph.\n",
    "            torch.Tensor: Labels for the graph.\n",
    "            int: Number of nodes in the graph.\n",
    "            int: Number of edges in the graph.\n",
    "            torch.Tensor: Edge adjacency matrix for the edges.\n",
    "        \"\"\"\n",
    "        \n",
    "        edge_index = torch.tensor(self.dataframe.iloc[idx]['edge_index'])\n",
    "        node_feat = torch.tensor(self.dataframe.iloc[idx]['node_feat'])\n",
    "        edge_attr = torch.tensor(self.dataframe.iloc[idx]['edge_attr'])\n",
    "        y = torch.tensor(self.dataframe.iloc[idx]['y'])\n",
    "        num_nodes = self.dataframe.iloc[idx]['num_nodes']\n",
    "        num_edges = self.dataframe.iloc[idx]['num_edges']\n",
    "        edge_adj = torch.tensor(self.dataframe.iloc[idx]['edge_adj'])\n",
    "\n",
    "        return edge_index, node_feat, edge_attr, y, num_nodes, num_edges, edge_adj\n",
    "    \n",
    "def custom_collate_2(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching graph data. Given a batch of graph samples, \n",
    "    it separates and returns seven lists.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of graph data, where each item is a tuple containing\n",
    "                      (edge_index, node_feat, edge_attr, y, num_nodes, num_edges, edge_adjacency) for a graph.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing seven lists, each containing the collected data from the batch:\n",
    "            - edge_index: List of adjency matrix\n",
    "            - node_feat: List of node features for each graph.\n",
    "            - edge_attr: List of edge attributes for each graph.\n",
    "            - y: List of target labels for each graph.\n",
    "            - num_nodes: List of the number of nodes for each graph.\n",
    "            - num_edges: List of the number of edges for each graph.\n",
    "            - edge_adjacency: List of adjency matrix for edges\n",
    "    \"\"\"    \n",
    "    edge_index = [item[0] for item in batch]\n",
    "    node_feat = [item[1] for item in batch]\n",
    "    edge_attr = [item[2] for item in batch]\n",
    "    y = [item[3] for item in batch]\n",
    "    num_nodes = [item[4] for item in batch]\n",
    "    num_edges = [item[5] for item in batch]\n",
    "    egde_ajd = [item[6] for item in batch]\n",
    "\n",
    "    \n",
    "    return edge_index, node_feat, edge_attr, y, num_nodes, num_edges, egde_ajd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset\n",
    "train_dataset = GraphDataset_2(train_set)\n",
    "test_dataset = GraphDataset_2(test_set)\n",
    "validation_dataset = GraphDataset_2(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1\n",
      "Batch Edge Index: torch.Size([17, 17])\n",
      "Batch Node Features: torch.Size([17, 7])\n",
      "Batch Edge Attributes: torch.Size([38, 4])\n",
      "Batch Labels: torch.Size([1])\n",
      "Batch Number of Nodes: 17\n",
      "Batch Number of Edges: 38\n",
      "Batch Edge Adjacency Matrix: torch.Size([38, 38])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader with custom collate function\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_2)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_2)\n",
    "\n",
    "# Inspect the first batch, but only the shapes this time\n",
    "\n",
    "for i, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes, batch_num_edges, batch_edge_adj) in enumerate(train_dataloader):\n",
    "    if i == 0:\n",
    "        print(\"Batch:\", i+1)\n",
    "        print(\"Batch Edge Index:\", batch_edge_index[0].shape)\n",
    "        print(\"Batch Node Features:\", batch_node_feat[0].shape)\n",
    "        print(\"Batch Edge Attributes:\", batch_edge_attr[0].shape)\n",
    "        print(\"Batch Labels:\", batch_y[0].shape)\n",
    "        print(\"Batch Number of Nodes:\", batch_num_nodes[0])\n",
    "        print(\"Batch Number of Edges:\", batch_num_edges[0])\n",
    "        print(\"Batch Edge Adjacency Matrix:\", batch_edge_adj[0].shape)\n",
    "        \n",
    "        # Stop after the first batch for demonstration\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the min number of features is the same as the max number of features. (with edges) (edge 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of features: 7\n",
      "Min number of features: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "max_features_node = 0\n",
    "min_features_node = 10000000000\n",
    "\n",
    "# Loop through each row in the 'node_feat' column\n",
    "for row in df['node_feat']:\n",
    "    # we suppose that inside the graph, all the nodes have the same number of features, we take only the number of features of the first node\n",
    "    num_features = len(row[0]) if row else 0 \n",
    "    # Update max and min number of features\n",
    "    max_features_node = max(max_features_node, num_features)\n",
    "    min_features_node = min(min_features_node, num_features)\n",
    "\n",
    "# we set the number of features to be the max number of features\n",
    "num_features_df = max_features_node\n",
    "\n",
    "print(\"Max number of features:\", max_features_node)\n",
    "print(\"Min number of features:\", min_features_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of features: 4\n",
      "Min number of features: 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "max_features_edge = 0\n",
    "min_features_edge = 10000000000\n",
    "\n",
    "# Loop through each row in the 'node_feat' column\n",
    "for row in df['edge_attr']:\n",
    "    # we suppose that inside the graph, all the nodes have the same number of features, we take only the number of features of the first node\n",
    "    num_features_edge = len(row[0]) if row else 0  # Check the length of the first inner list\n",
    "    # Update max and min number of features\n",
    "    max_features_edge = max(max_features_edge, num_features_edge)\n",
    "    min_features_edge = min(min_features_edge, num_features_edge)\n",
    "\n",
    "num_features_edge_df = max_features_edge\n",
    "print(\"Max number of features:\", max_features_edge)\n",
    "print(\"Min number of features:\", min_features_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data type that can be send to pytorch and be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define graph neural network that will predict the mutagenicity\n",
    "\n",
    "We define a GNN that will be trained which correspond to this part in the picture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphNetwork_2(nn.Module):\n",
    "    def __init__(self, num_in_features_node, num_out_features_node, num_in_features_edge, num_out_features_edge, conv_layers_node, conv_layers_edge, aggregation_node, aggregation_edge, number_of_hidden_neurons = []):\n",
    "        \"\"\"\n",
    "        Initialize the custom graph neural network.\n",
    "\n",
    "        Args:\n",
    "            num_in_features (int): Number of input node features.\n",
    "            num_out_features (int): Number of output features for each convolutional layer.\n",
    "            conv_layers (nn.ModuleList): List of graph convolutional layer instances.\n",
    "            aggregation (nn.Module): Instance of pooling layer.\n",
    "            hidden_layer_sizes (list): List of hidden layer sizes for fully connected layers.\n",
    "        \"\"\"       \n",
    "        super(CustomGraphNetwork_2, self).__init__()\n",
    "        # Infeatures: number of input features per node\n",
    "        self.num_in_features_node = num_in_features_node\n",
    "        # Outfeatures: number of output features per node\n",
    "        self.num_out_features_node = num_out_features_node\n",
    "        # Infeatures: number of input features per edge\n",
    "        self.num_in_features_edge = num_in_features_edge\n",
    "        # Outfeatures: number of output features per edge\n",
    "        self.num_out_features_edge = num_out_features_edge\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers_node = nn.ModuleList(conv_layers_node)\n",
    "        self.conv_layers_edge = nn.ModuleList(conv_layers_edge)\n",
    "        \n",
    "        # Aggregation function: which is applied to aggregate into a shape 1, out_features        \n",
    "        self.aggregation_node = aggregation_node\n",
    "        self.aggregation_edge = aggregation_edge\n",
    "        \n",
    "        # list of hidden layer sizes for fully connected layers\n",
    "        self.number_of_hidden_neurons = number_of_hidden_neurons\n",
    "        self.fully_connected = nn.ModuleList()\n",
    "        in_features_of_fully_connected = self.num_out_features_node + self.num_out_features_edge\n",
    "        # build of the fully connected layers\n",
    "        for i in number_of_hidden_neurons:\n",
    "            self.fully_connected.append(nn.Linear(in_features_of_fully_connected, i))\n",
    "            in_features_of_fully_connected = i\n",
    "        # add the last fully connected layer\n",
    "        self.fully_connected.append(nn.Linear(in_features_of_fully_connected, 2))    \n",
    "        \n",
    "    def forward(self, x,adj, edge_x, edge_adj):\n",
    "        \"\"\"\n",
    "        Forward pass through the custom graph neural network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features, shape (num_nodes, num_in_features).\n",
    "            adj (Tensor): Adjacency matrix, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: with the probality of each graph to be in the class 0 or 1\n",
    "        \"\"\"\n",
    "        # Pass input through each graph convolutional layer for the nodes   \n",
    "        for conv_layer_node in self.conv_layers_node:\n",
    "            x = conv_layer_node(x, adj)  # Pass adjacency matrix to the convolutional layer\n",
    "        node_graph_output = x\n",
    "        \n",
    "        \n",
    "        # Pass input through each graph convolutional layer for the edges   \n",
    "        for conv_layer_edge in self.conv_layers_edge:\n",
    "            edge_x = conv_layer_edge(edge_x, edge_adj)  # Pass adjacency matrix to the convolutional layer\n",
    "        edge_graph_output = edge_x\n",
    "\n",
    "        \n",
    "        # Apply global pooling along the node dimension (dimension 1)\n",
    "        if self.aggregation_node == \"mean\" :\n",
    "            node_graph_output = torch.mean(node_graph_output, dim=0, keepdim=False)\n",
    "        elif self.aggregation_node == \"max\" :\n",
    "            node_graph_output, _ = torch.max(node_graph_output, dim=0, keepdim=False)\n",
    "        else :\n",
    "            raise ValueError(\"Aggregation_node type not supported.\")\n",
    "        \n",
    "        # Apply global pooling along the edge dimension (dimension 1)\n",
    "        if self.aggregation_edge == \"mean\" :\n",
    "            edge_graph_output = torch.mean(edge_graph_output, dim=0, keepdim=False)\n",
    "        elif self.aggregation_edge == \"max\" :\n",
    "            edge_graph_output, _ = torch.max(edge_graph_output, dim=0, keepdim=False)\n",
    "            \n",
    "        else :\n",
    "            raise ValueError (\"Aggregation_edge type not supported.\")\n",
    "        \n",
    "        # Concatenate the node and edge graph outputs\n",
    "        graph_output = torch.cat((node_graph_output, edge_graph_output), dim=0)\n",
    "              \n",
    "\n",
    "        # Apply the fully connected layers    \n",
    "        for fc_layer in self.fully_connected:\n",
    "            graph_output = fc_layer(graph_output)\n",
    "            \n",
    "            \n",
    "        # Apply softmax activation\n",
    "        softmax_output = torch.softmax(graph_output, dim=0)\n",
    "\n",
    "        return softmax_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with edge features solution 2, , create a function that will train the model and return the loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefine the training function for egde consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_2(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a GNN using a training loop.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be trained.\n",
    "    - optimizer: The optimizer for updating model parameters.\n",
    "    - criterion: The loss function used for optimization.\n",
    "    - train_dataloader: DataLoader providing training data.\n",
    "    - num_epochs: The number of training epochs.\n",
    "    - learning_rate: The learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    - trained_model: The trained machine learning model.\n",
    "    - training_accuracy: The overall training accuracy across all epochs.\n",
    "    - losses: A list of losses during training.\n",
    "    \"\"\"\n",
    "    # Initialize variables for training accuracy and losses    \n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "    losses = []\n",
    "    \n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize variables for training accuracy and losses for each epoch\n",
    "        sub_accuracy_list = []\n",
    "        correct_train_predictions_epoch = 0\n",
    "        total_train_samples_epoch = 0\n",
    "\n",
    "        # Loop through each batch in the training dataloader\n",
    "        for idx, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes, batch_num_edges, batch_edge_adj) in enumerate(train_dataloader):\n",
    "           \n",
    "            # loop through each graph in the batch\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                \n",
    "                # Extract features, adjency matrix and labels for the current graph\n",
    "                node_features = batch_node_feat[i].float()\n",
    "                edge_features = batch_edge_attr[i].float()\n",
    "                adj_matrix = batch_edge_index[i].float()\n",
    "                adj_matrix_edge = batch_edge_adj[i].float()\n",
    "                ground_truth_labels = batch_y[i].clone()\n",
    "                \n",
    "                # Convert to one-hot labels\n",
    "                one_hot_labels = torch.zeros((ground_truth_labels.size(0), 2))\n",
    "                one_hot_labels.scatter_(1, ground_truth_labels.view(-1, 1), 1).squeeze_()\n",
    "               \n",
    "                # Forward pass\n",
    "                output = model(node_features, adj_matrix, edge_features, adj_matrix_edge)   \n",
    "                \n",
    "                # Calculate the predicted class based on probability threshold (e.g., 0.5)\n",
    "                predicted_class = (output > 0.5).long()\n",
    "                condition = (predicted_class == one_hot_labels).all().item()\n",
    "                # condition : if the prediction == ground truth -> add 1 to the correct_train_predictions\n",
    "                if condition:\n",
    "                    correct_train_predictions += 1\n",
    "                    correct_train_predictions_epoch = correct_train_predictions_epoch + 1\n",
    "                #Calculate the total number of samples\n",
    "                total_train_samples += 1\n",
    "                total_train_samples_epoch = total_train_samples_epoch + 1  \n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                # transform the output to probabilities for loss calculation\n",
    "                output_probabilities = output[1]  # Select the second element (probability for class 1)\n",
    "                ground_truth_labels =  ground_truth_labels.squeeze()\n",
    "                output_logits = torch.log(output_probabilities / (1 - output_probabilities))\n",
    "                # Calculate the loss\n",
    "                loss = criterion(output_logits, ground_truth_labels.float())\n",
    "                losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "                # Backpropagation\n",
    "                #optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # optimizer.step()\n",
    "                # Optimizer step does not work, Update weights using gradient descent we do this manually because I do not understand why optimizer.step() does not work :@              \n",
    "                with torch.no_grad(): \n",
    "                    for name, param in model.named_parameters():\n",
    "                        param -= learning_rate * param.grad\n",
    "                            \n",
    "\n",
    "        # Calculate training accuracy\n",
    "        training_accuracy = (correct_train_predictions_epoch / total_train_samples_epoch) * 100\n",
    "        sub_accuracy_list.append(training_accuracy)\n",
    "        \n",
    "        # Print training accuracy for each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Accuracy: {training_accuracy:.2f}%\")\n",
    "\n",
    "        #Calculate the mean of the accuracy for all the epochs\n",
    "        training_accuracy_final = (correct_train_predictions / total_train_samples) * 100\n",
    "\n",
    "    return model, training_accuracy_final, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate with edges features 2, create a function that will validate the model and return the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_2(model, validation_dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Validate a GNN using a validation loop.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be trained.\n",
    "    - validation_dataloader: DataLoader providing training data.\n",
    "    - criterion: The loss function used for optimization.\n",
    "\n",
    "    Returns:\n",
    "    - average_validation_loss: the average validation loss\n",
    "    - validation_accuracy: The overall validation accuracy across all epochs.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (no gradient updates)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize variables for validation accuracy and loss\n",
    "    # We have for each two numbers so we can calculate the mean of the accuracy and loss\n",
    "    total_validation_loss = 0.0\n",
    "    validation_batches = 0\n",
    "\n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for idx, (batch_edge_index, batch_node_feat, batch_edge_attr, batch_y, batch_num_nodes, batch_num_edges, batch_edge_adj) in enumerate(validation_dataloader):\n",
    "            for i in range(len(batch_edge_index)):\n",
    "                \n",
    "                # Extract features, adjency matrix and labels for the current graph\n",
    "                node_features = batch_node_feat[i].float()\n",
    "                edge_features = batch_edge_attr[i].float()\n",
    "                adj_matrix = batch_edge_index[i].float()\n",
    "                adj_matrix_edge = batch_edge_adj[i].float()\n",
    "                ground_truth_labels = batch_y[i].clone().detach()\n",
    "\n",
    "                # Convert labels to one-hot labels\n",
    "                one_hot_labels = torch.zeros((ground_truth_labels.size(0), 2))\n",
    "                one_hot_labels.scatter_(1, ground_truth_labels.view(-1, 1), 1).squeeze_()\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(node_features, adj_matrix, edge_features, adj_matrix_edge)\n",
    "\n",
    "                # Calculate the predicted class based on probability of the output\n",
    "                predicted_class = (output > 0.5).long()  # Assuming class 1 corresponds to index 1\n",
    "                condition = (predicted_class == one_hot_labels).all().item()\n",
    "                # condition : if the prediction == ground truth -> add 1 to the correct_train_predictions\n",
    "                if condition:\n",
    "                    correct_train_predictions += 1\n",
    "                total_train_samples += 1\n",
    "\n",
    "                # Compute the validation loss (no backpropagation)\n",
    "                validation_loss = criterion(output, one_hot_labels)\n",
    "\n",
    "                # add the validation loss to the total validation loss\n",
    "                total_validation_loss += validation_loss.item()\n",
    "                validation_batches += 1\n",
    "\n",
    "    # Calculate the average validation loss\n",
    "    average_validation_loss = total_validation_loss / validation_batches\n",
    "    # Calculate validation accuracy\n",
    "    validation_accuracy = (correct_train_predictions / total_train_samples) * 100\n",
    "\n",
    "    return average_validation_loss, validation_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare informations for multiple training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the number of features that we want to put in the model and the number of features that is going out of the model\n",
    "# We consider that they remain the same\n",
    "\n",
    "in_features_node= max_features_node\n",
    "out_features_node = max_features_node #can be different\n",
    "in_features_edge= max_features_edge\n",
    "out_features_edge = max_features_edge #can be different\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.18%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 70.23%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 69.47%\n",
      "Epoch [6/20], Training Accuracy: 80.92%\n",
      "Epoch [7/20], Training Accuracy: 80.15%\n",
      "Epoch [8/20], Training Accuracy: 81.68%\n",
      "Epoch [9/20], Training Accuracy: 81.68%\n",
      "Epoch [10/20], Training Accuracy: 80.92%\n",
      "Epoch [11/20], Training Accuracy: 81.68%\n",
      "Epoch [12/20], Training Accuracy: 82.44%\n",
      "Epoch [13/20], Training Accuracy: 79.39%\n",
      "Epoch [14/20], Training Accuracy: 77.10%\n",
      "Epoch [15/20], Training Accuracy: 81.68%\n",
      "Epoch [16/20], Training Accuracy: 80.15%\n",
      "Epoch [17/20], Training Accuracy: 74.05%\n",
      "Epoch [18/20], Training Accuracy: 74.05%\n",
      "Epoch [19/20], Training Accuracy: 81.68%\n",
      "Epoch [20/20], Training Accuracy: 81.68%\n",
      "Training loss: 0.2303\n",
      "Training accuracy final: 77.02%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers for nodes, we can stack mutiple layers in a list\n",
    "conv_layers_node = [GraphConv(in_features_node, out_features_node, \"max\", activation=nn.ReLU())]\n",
    "# Define the model for the convolutional layers for edges, we can stack mutiple layers in a list\n",
    "conv_layers_edge = [GraphConv(in_features_edge, out_features_edge, \"max\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork_2(in_features_node, out_features_node, in_features_edge, out_features_edge, conv_layers_node, conv_layers_edge, \"max\", \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model_2(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6450578488152603\n",
      "Validation Accuracy: 79.31%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model_2(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.18%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 67.18%\n",
      "Epoch [8/20], Training Accuracy: 67.18%\n",
      "Epoch [9/20], Training Accuracy: 67.18%\n",
      "Epoch [10/20], Training Accuracy: 69.47%\n",
      "Epoch [11/20], Training Accuracy: 68.70%\n",
      "Epoch [12/20], Training Accuracy: 67.18%\n",
      "Epoch [13/20], Training Accuracy: 70.99%\n",
      "Epoch [14/20], Training Accuracy: 70.99%\n",
      "Epoch [15/20], Training Accuracy: 72.52%\n",
      "Epoch [16/20], Training Accuracy: 74.05%\n",
      "Epoch [17/20], Training Accuracy: 74.05%\n",
      "Epoch [18/20], Training Accuracy: 74.05%\n",
      "Epoch [19/20], Training Accuracy: 72.52%\n",
      "Epoch [20/20], Training Accuracy: 71.76%\n",
      "Training loss: 0.6529\n",
      "Training accuracy final: 69.54%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers for nodes, we can stack mutiple layers in a list\n",
    "conv_layers_node = [GraphSAGEConv(in_features_node, out_features_node, \"max\", activation=nn.ReLU())]\n",
    "# Define the model for the convolutional layers for edges, we can stack mutiple layers in a list\n",
    "conv_layers_edge = [GraphSAGEConv(in_features_edge, out_features_edge, \"max\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork_2(in_features_node, out_features_node, in_features_edge, out_features_edge, conv_layers_node, conv_layers_edge, \"max\", \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model_2(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.672813238768742\n",
      "Validation Accuracy: 68.97%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model_2(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 67.18%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 67.18%\n",
      "Epoch [8/20], Training Accuracy: 67.18%\n",
      "Epoch [9/20], Training Accuracy: 67.18%\n",
      "Epoch [10/20], Training Accuracy: 67.18%\n",
      "Epoch [11/20], Training Accuracy: 67.18%\n",
      "Epoch [12/20], Training Accuracy: 67.18%\n",
      "Epoch [13/20], Training Accuracy: 67.18%\n",
      "Epoch [14/20], Training Accuracy: 67.18%\n",
      "Epoch [15/20], Training Accuracy: 67.18%\n",
      "Epoch [16/20], Training Accuracy: 67.18%\n",
      "Epoch [17/20], Training Accuracy: 67.18%\n",
      "Epoch [18/20], Training Accuracy: 67.18%\n",
      "Epoch [19/20], Training Accuracy: 67.18%\n",
      "Epoch [20/20], Training Accuracy: 67.18%\n",
      "Training loss: 0.4851\n",
      "Training accuracy final: 67.18%\n"
     ]
    }
   ],
   "source": [
    "# Define the model for the convolutional layers for nodes, we can stack mutiple layers in a list\n",
    "conv_layers_node = [AttentionConv(in_features_node, out_features_node, \"sum\", activation=nn.ReLU())]\n",
    "# Define the model for the convolutional layers for edges, we can stack mutiple layers in a list\n",
    "conv_layers_edge = [AttentionConv(in_features_edge, out_features_edge, \"sum\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork_2(in_features_node, out_features_node, in_features_edge, out_features_edge, conv_layers_node, conv_layers_edge, \"max\", \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model_2(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.706594300681147\n",
      "Validation Accuracy: 65.52%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model_2(model, validation_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test finaly the best model on the test set, the best results are obtained with the GraphConv model, with a learning rate of 1E-4, using the second method for the edge intergration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Accuracy: 64.89%\n",
      "Epoch [2/20], Training Accuracy: 67.18%\n",
      "Epoch [3/20], Training Accuracy: 67.18%\n",
      "Epoch [4/20], Training Accuracy: 67.18%\n",
      "Epoch [5/20], Training Accuracy: 67.18%\n",
      "Epoch [6/20], Training Accuracy: 67.18%\n",
      "Epoch [7/20], Training Accuracy: 67.18%\n",
      "Epoch [8/20], Training Accuracy: 67.18%\n",
      "Epoch [9/20], Training Accuracy: 67.94%\n",
      "Epoch [10/20], Training Accuracy: 73.28%\n",
      "Epoch [11/20], Training Accuracy: 73.28%\n",
      "Epoch [12/20], Training Accuracy: 74.81%\n",
      "Epoch [13/20], Training Accuracy: 70.23%\n",
      "Epoch [14/20], Training Accuracy: 74.81%\n",
      "Epoch [15/20], Training Accuracy: 73.28%\n",
      "Epoch [16/20], Training Accuracy: 70.99%\n",
      "Epoch [17/20], Training Accuracy: 75.57%\n",
      "Epoch [18/20], Training Accuracy: 69.47%\n",
      "Epoch [19/20], Training Accuracy: 69.47%\n",
      "Epoch [20/20], Training Accuracy: 74.05%\n",
      "Training loss: 0.2237\n",
      "Training accuracy final: 70.11%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "# Define the model for the convolutional layers for nodes, we can stack mutiple layers in a list\n",
    "conv_layers_node = [GraphConv(in_features_node, out_features_node, \"max\", activation=nn.ReLU())]\n",
    "# Define the model for the convolutional layers for edges, we can stack mutiple layers in a list\n",
    "conv_layers_edge = [GraphConv(in_features_edge, out_features_edge, \"max\", activation=nn.ReLU())]\n",
    "# Define the model of the GNN\n",
    "model = CustomGraphNetwork_2(in_features_node, out_features_node, in_features_edge, out_features_edge, conv_layers_node, conv_layers_edge, \"max\", \"max\",[])\n",
    "# Define the optimizer, which is used to update the weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model using the training loop\n",
    "model, training_accuracy_final, losses = train_model_2(model, optimizer, criterion, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "#print the loss and accuracy during the training process\n",
    "print(f\"Training loss: {losses[-1]:.4f}\")\n",
    "print(f\"Training accuracy final: {training_accuracy_final:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final test with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6438114430223193\n",
      "Validation Accuracy: 82.14%\n"
     ]
    }
   ],
   "source": [
    "# Print the final loss and the final accuracy\n",
    "average_loss, accuracy = validate_model_2(model, test_dataloader, criterion)\n",
    "print(f\"Validation Loss: {average_loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only obtained an accuracy of 82.1 % which is good but not great."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
